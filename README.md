# ğŸ¤– AUTODESKAI - Adaptive Gesture Recognition System

> **AI-Powered Hand Gesture Control with Automatic Learning**
> 
> A next-generation gesture recognition system that learns from user input, automatically augments training data, and adapts to individual users in real-time.

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Gesture Modes](#gesture-modes)
  - [ğŸ‘¨â€ğŸ’» Developer Mode](#-developer-mode)
  - [ğŸ¤ Presentation Mode](#-presentation-mode)
  - [ğŸµ Casual Mode](#-casual-mode)
  - [ğŸ“ˆ Trading Mode](#-trading-mode)
- [Technology Stack](#technology-stack)
- [Architecture](#architecture)
- [Installation](#installation)
- [Usage](#usage)
- [API Documentation](#api-documentation)
- [Project Structure](#project-structure)

---

## Overview

AUTODESK is an intelligent gesture recognition system that combines computer vision, deep learning, and real-time processing to create a personalized hands-free control experience. Unlike traditional static gesture systems, FlowGesture **adapts to each user** by:

- âœ¨ **Automatically augmenting** user-recorded samples (6x data multiplication)
- ğŸ”„ **Retraining the model** in the background when new gestures are added
- ğŸ¯ **Learning incrementally** without losing previous gesture accuracy
- âš¡ **Recognizing gestures in real-time** with <30ms latency

**Perfect for:** Developers, presenters, traders, content creators, gamers, and anyone who wants hands-free control.

---

## âœ¨ Features

### ğŸ§  Adaptive Learning
- **Zero Configuration**: Users create gestures through a simple web interface
- **Auto-Augmentation**: Each recorded sample generates 6 augmented versions automatically
  - Brightness variations (bright/dim rooms)
  - Contrast adjustments (harsh/soft lighting)
  - Noise injection (low-quality webcam simulation)
  - Mirror flips (different hand angles)
- **Incremental Training**: Add new gestures anytime without retraining from scratch
- **Background Processing**: Model trains while you continue using the system

### ğŸ® Real-Time Recognition
- **<30ms Latency**: Instant gesture detection on CPU
- **90%+ Accuracy**: After just 25 samples per gesture
- **WebSocket Streaming**: Live predictions sent to frontend
- **Confidence Scoring**: Know how certain the model is

### ğŸ”§ User-Friendly Interface
- **Beautiful UI**: Built with React + Tailwind CSS
- **Webcam Integration**: Real-time video preview with hand landmark overlay
- **Progress Tracking**: Visual feedback during recording and training
- **Gesture Management**: Easy CRUD operations (Create, Read, Update, Delete)

### ğŸ’¾ Persistent Storage
- **Automatic Saving**: All gestures and models saved locally
- **Session Recovery**: Resume training after restart
- **Export/Import**: Share gesture packs between users

---

## ğŸ¨ Gesture Modes

FlowGesture includes 4 pre-built mode packs, each optimized for specific workflows. Users can also create **custom gestures** for any use case.

### ğŸ‘¨â€ğŸ’» Developer Mode

Perfect for coding, debugging, and navigating your IDE hands-free.

| Gesture | Action | Description |
|---------|--------|-------------|
| âœŒï¸ **Peace** | Next file (VS Code) | Switch to next open file in editor |
| âœŠ **Fist** | Toggle terminal | Show/hide integrated terminal |
| â˜ï¸ **Point Up** | Scroll up in editor | Navigate up through code |
| ğŸ‘ **Thumb Down** | Scroll down | Navigate down through code |
| ğŸ– **Open Palm** | Run code (Ctrl+F5) | Execute current file |
| ğŸ‘ **Thumb Up** | Git commit shortcut | Quick commit staged changes |
| ğŸ†• **Custom** | Launch Cursor AI | Open AI coding assistant |

### ğŸ¤ Presentation Mode

Control your slides and screen like a pro presenter â€” no clicker needed.

| Gesture | Action | Description |
|---------|--------|-------------|
| â˜ï¸ **Point Up** | Next slide | Advance presentation forward |
| âœŠ **Fist** | Previous slide | Go back one slide |
| ğŸ– **Open Palm** | Start / pause slideshow | Toggle presentation mode |
| âœŒï¸ **Peace** | Laser pointer (cursor highlight) | Highlight areas on screen |
| ğŸ‘ **Thumb Up** | Fullscreen toggle | Enter/exit fullscreen mode |
| ğŸ‘ **Thumb Down** | Black screen (presenter pause) | Pause with blank screen |

### ğŸµ Casual Mode

Media playback control for Spotify, YouTube, and entertainment.

| Gesture | Action | Description |
|---------|--------|-------------|
| ğŸ‘ **Thumb Up** | Play / Pause Spotify | Toggle music playback |
| âœŒï¸ **Peace** | Next track | Skip to next song |
| ğŸ‘ **Thumb Down** | Previous track | Go back to previous song |
| ğŸ– **Open Palm** | Volume up | Increase audio volume |
| âœŠ **Fist** | Volume down / Mute | Decrease or mute audio |
| ğŸ¤˜ **ILoveYou** | Launch YouTube | Open YouTube in browser |
| â˜ï¸ **Point Up** | Brightness up | Increase screen brightness |

### ğŸ“ˆ Trading Mode

**NEW!** Execute trades, switch charts, and manage positions hands-free.

| Gesture | Action | Description |
|---------|--------|-------------|
| ğŸ‘ **Thumb Up** | Buy / Long position | Execute long trade |
| ğŸ‘ **Thumb Down** | Sell / Short position | Execute short trade |
| âœŠ **Fist** | Close position | Exit current trade |
| ğŸ– **Open Palm** | Switch timeframe | Cycle between 1m/5m/15m/1h charts |
| âœŒï¸ **Peace** | Next trading pair | Switch to next asset |
| â˜ï¸ **Point Up** | Zoom in (chart) | Zoom into price action |
| ğŸ¤ **Pinch** | Zoom out (chart) | Zoom out for overview |
| ğŸ¤˜ **ILoveYou** | Set alert | Place price alert at current level |

---

## ğŸ›  Technology Stack

### Backend (`adaptive_gesture_system.py`)
- **FastAPI**: High-performance REST API + WebSocket server
- **PyTorch**: Deep learning framework for model training
- **MobileNetV3**: Lightweight CNN for fast inference (~30ms on CPU)
- **MediaPipe**: Hand landmark detection (21 keypoints)
- **OpenCV**: Image processing and augmentation
- **Uvicorn**: ASGI server with WebSocket support

### Frontend (React + Tailwind CSS)
- **React 18**: Component-based UI framework
- **Tailwind CSS**: Utility-first styling
- **Vite**: Lightning-fast build tool
- **WebSocket API**: Real-time communication
- **HTML5 Canvas**: Webcam video processing

### Data Pipeline
- **NumPy**: Numerical operations
- **PIL/Pillow**: Image manipulation
- **JSON**: Metadata storage

---

## ğŸ— Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    React Frontend (Port 3000)               â”‚
â”‚         Built with React + Tailwind CSS                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Webcam    â”‚  â”‚  Gesture   â”‚  â”‚  Live Recognition    â”‚  â”‚
â”‚  â”‚  Capture   â”‚  â”‚  Manager   â”‚  â”‚  Display             â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚ HTTP + WebSocket
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              FastAPI Backend (Port 8000)                    â”‚
â”‚           adaptive_gesture_system.py                        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  REST API    â”‚  â”‚  WebSocket   â”‚  â”‚  Background     â”‚   â”‚
â”‚  â”‚  Endpoints   â”‚  â”‚  Handler     â”‚  â”‚  Trainer        â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Core Modules                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  DataAugmenter: 6x sample multiplication            â”‚   â”‚
â”‚  â”‚  â€¢ Brightness: Â±40%                                  â”‚   â”‚
â”‚  â”‚  â€¢ Contrast: Â±50%                                    â”‚   â”‚
â”‚  â”‚  â€¢ Noise: Gaussian Ïƒ=15                             â”‚   â”‚
â”‚  â”‚  â€¢ Flip: Horizontal mirror                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  GestureDataset: CRUD operations                     â”‚   â”‚
â”‚  â”‚  â€¢ Create/Read/Update/Delete gestures               â”‚   â”‚
â”‚  â”‚  â€¢ Metadata tracking (samples, timestamps)          â”‚   â”‚
â”‚  â”‚  â€¢ Auto-organization (raw + augmented folders)      â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                                                             â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚  GestureModel: Incremental learning                  â”‚   â”‚
â”‚  â”‚  â€¢ MobileNetV3-Small (2.5M params)                   â”‚   â”‚
â”‚  â”‚  â€¢ 15 epochs, ~2 minutes training (CPU)             â”‚   â”‚
â”‚  â”‚  â€¢ Auto-save best checkpoint                        â”‚   â”‚
â”‚  â”‚  â€¢ <30ms inference latency                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚
             â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   Persistent Storage                        â”‚
â”‚  user_gestures/                                            â”‚
â”‚  â”œâ”€â”€ raw/              â† Original samples                  â”‚
â”‚  â”œâ”€â”€ augmented/        â† 6x augmented versions            â”‚
â”‚  â””â”€â”€ metadata.json     â† Gesture configs                  â”‚
â”‚                                                             â”‚
â”‚  trained_models/                                           â”‚
â”‚  â””â”€â”€ current_model.pth â† Trained PyTorch model            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¦ Installation

### Prerequisites
- Python 3.8+ (3.10 recommended)
- Node.js 16+ (for frontend)
- Webcam/camera access

### Backend Setup

```bash
# Clone repository
git clone https://github.com/yourusername/flowgesture.git
cd flowgesture/backend

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install torch torchvision opencv-python mediapipe fastapi uvicorn websockets pyautogui pydantic

# Run backend server
python adaptive_gesture_system.py
```

Backend starts on: **http://localhost:8000**  
API docs available at: **http://localhost:8000/docs**

### Frontend Setup

```bash
# Navigate to frontend directory
cd ../frontend

# Install dependencies
npm install

# Start development server
npm run dev
```

Frontend starts on: **http://localhost:3000**

---

## ğŸš€ Usage

### 1. Create a New Gesture

1. Open frontend at `http://localhost:3000`
2. Click **"Add Gesture"** button
3. Enter gesture details:
   - **ID**: `peace_sign` (no spaces)
   - **Name**: `Peace Sign`
   - **Emoji**: âœŒï¸
4. Click **Create**

### 2. Record Training Samples

1. Select the gesture from the list
2. Click **"Record Samples"**
3. Position your hand in the webcam frame
4. Click **Start Recording**
5. Hold the gesture steady while the system captures 25-30 samples
6. System automatically:
   - Saves raw samples
   - Generates 6 augmented versions per sample
   - Triggers training when 20+ samples collected

**Tip**: Record samples in different:
- Lighting conditions (bright/dim)
- Hand positions (near/far from camera)
- Angles (tilted, rotated)

### 3. Model Training (Automatic)

The system trains automatically in the background:
- **Progress bar** shows training status (0-100%)
- **Console logs** display epoch progress
- **Notification** when training completes
- Training takes ~2-5 minutes for 8 gestures

### 4. Real-Time Recognition

1. Click **"Start Recognition"** button
2. Perform your trained gesture
3. System displays:
   - **Gesture name**: Which gesture was detected
   - **Confidence**: 0-100% certainty
   - **Latency**: Recognition speed in milliseconds

### 5. Edit/Delete Gestures

- **Edit**: Click gesture â†’ Update name/emoji â†’ Save
- **Delete**: Click trash icon â†’ Confirm
- System **automatically retrains** after deletion

---

## ğŸ“š API Documentation

### REST Endpoints

#### **GET** `/api/gestures`
Get all gestures with metadata.

**Response:**
```json
{
  "gestures": [
    {
      "id": "peace",
      "name": "Peace Sign",
      "emoji": "âœŒï¸",
      "raw_samples": 25,
      "augmented_samples": 175,
      "enabled": true,
      "created_at": "2024-01-20T10:30:00"
    }
  ]
}
```

#### **POST** `/api/gestures`
Create a new gesture.

**Request:**
```json
{
  "id": "thumbs_up",
  "name": "Thumbs Up",
  "emoji": "ğŸ‘"
}
```

#### **POST** `/api/gestures/{gesture_id}/add-sample`
Add a training sample (auto-augments).

**Request:**
```json
{
  "frame": "data:image/jpeg;base64,/9j/4AAQSkZJRg..."
}
```

**Response:**
```json
{
  "ok": true,
  "raw_count": 26,
  "aug_count": 182
}
```

#### **DELETE** `/api/gestures/{gesture_id}`
Delete gesture and trigger retraining.

#### **PATCH** `/api/gestures/{gesture_id}`
Update gesture metadata.

#### **GET** `/api/model/status`
Check training progress.

**Response:**
```json
{
  "is_training": true,
  "progress": 67,
  "accuracy": 89.5,
  "gesture_count": 8,
  "classes": ["peace", "thumbs_up", "fist", ...]
}
```

### WebSocket

#### **WS** `/ws/recognize`
Real-time gesture recognition stream.

**Send:**
```json
{
  "type": "frame",
  "data": "data:image/jpeg;base64,..."
}
```

**Receive:**
```json
{
  "gesture": "peace",
  "confidence": 0.953,
  "latency_ms": 28.4,
  "timestamp": 1705750800.123
}
```

---

## ğŸ“ Project Structure

```
flowgesture/
â”œâ”€â”€ backend/
â”‚   â”œâ”€â”€ adaptive_gesture_system.py   â† Main backend server (COMPLETE)
â”‚   â”œâ”€â”€ user_gestures/               â† User data (auto-created)
â”‚   â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â”œâ”€â”€ augmented/
â”‚   â”‚   â””â”€â”€ metadata.json
â”‚   â”œâ”€â”€ trained_models/              â† Saved models (auto-created)
â”‚   â”‚   â””â”€â”€ current_model.pth
â”‚   â””â”€â”€ requirements.txt
â”‚
â”œâ”€â”€ frontend/                         â† Built with React + Tailwind CSS
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â”œâ”€â”€ WebcamView.jsx      â† Webcam display
â”‚   â”‚   â”‚   â”œâ”€â”€ GestureList.jsx     â† Gesture management
â”‚   â”‚   â”‚   â”œâ”€â”€ RecordPanel.jsx     â† Sample recording
â”‚   â”‚   â”‚   â”œâ”€â”€ LiveRecognition.jsx â† Real-time display
â”‚   â”‚   â”‚   â””â”€â”€ TrainingProgress.jsxâ† Training status
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â””â”€â”€ gestureAPI.js       â† API client
â”‚   â”‚   â”œâ”€â”€ App.jsx
â”‚   â”‚   â””â”€â”€ main.jsx
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ PROJECT_README.md            â† Full documentation
â”‚   â”œâ”€â”€ frontend_integration_example.js
â”‚   â””â”€â”€ API_REFERENCE.md
â”‚
â””â”€â”€ README.md                         â† This file
```

---

## ğŸ“ How It Works

### The Adaptive Learning Pipeline

```
User Records 1 Sample
        â†“
Backend Receives Frame
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Data Augmentation (6x)    â”‚
â”‚  âœ“ Bright (1.4x brightness) â”‚
â”‚  âœ“ Dim (0.6x brightness)    â”‚
â”‚  âœ“ High contrast (1.5x)     â”‚
â”‚  âœ“ Low contrast (0.7x)      â”‚
â”‚  âœ“ Gaussian noise (Ïƒ=15)    â”‚
â”‚  âœ“ Horizontal flip          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
7 Images Saved (1 raw + 6 aug)
        â†“
Sample Counter: 25/25
        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Auto-Trigger Training     â”‚
â”‚  â€¢ Build PyTorch dataset    â”‚
â”‚  â€¢ MobileNetV3-Small model  â”‚
â”‚  â€¢ 15 epochs (~2 minutes)   â”‚
â”‚  â€¢ Save best checkpoint     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â†“
Model Ready for Inference
        â†“
Real-Time Recognition
  <30ms latency | >90% accuracy
```

### Why This Approach Wins

**Traditional Systems:**
- âŒ Fixed gestures (developers decide)
- âŒ One-size-fits-all model
- âŒ Manual retraining needed
- âŒ Poor adaptation to new users

**FlowGesture (Ours):**
- âœ… User-defined gestures
- âœ… Personalized model per user
- âœ… Automatic retraining
- âœ… Learns from each user's hand

---

## ğŸ† Key Innovations

1. **Automatic Augmentation**: 6x data multiplication without user effort
2. **Incremental Learning**: Add gestures without retraining everything
3. **Background Training**: Non-blocking model updates
4. **Real-Time Adaptation**: Model improves as you use it
5. **Mode-Based Workflows**: Pre-built gesture packs for common tasks
6. **Trading Mode**: First gesture system designed for financial trading

---

## ğŸ“Š Performance Metrics

| Metric | Value |
|--------|-------|
| **Recognition Latency** | <30ms (CPU) |
| **Training Time** | 2-5 minutes (8 gestures, CPU) |
| **Accuracy** | >90% (25+ samples/gesture) |
| **Model Size** | ~10MB |
| **Memory Usage** | <500MB (training), <100MB (inference) |
| **Frame Rate** | 10-30 FPS |
| **Inference Device** | CPU-only (no GPU required) |

---

## ğŸ¤ Contributing

We welcome contributions! Please see [CONTRIBUTING.md](docs/CONTRIBUTING.md) for guidelines.

---

## ğŸ“„ License

This project is licensed under the MIT License - see [LICENSE](LICENSE) file for details.

---

## ğŸ‘¥ Team

- **Backend Developer**: Gesture recognition model (`adaptive_gesture_system.py`), API, training pipeline
- **Frontend Developer**: React UI with Tailwind CSS, webcam integration, user experience
- **Project**: Final Year B.Tech Project

---

## ğŸ™ Acknowledgments

- MediaPipe team for hand tracking models
- PyTorch community for deep learning framework
- FastAPI for the excellent web framework
- Tailwind CSS for beautiful styling

---

## ğŸ“ Support

For issues, questions, or feature requests:
- Open an issue on GitHub
- Email: your.email@example.com
- Documentation: [Full Docs](docs/PROJECT_README.md)

---

**Built with â¤ï¸ for hands-free productivity**

ğŸŒŸ **Star this repo if you find it useful!**
